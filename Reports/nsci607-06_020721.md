<!-- ---
title: NSCI607-06
author: Trevor Martin's Notes
date: Feb. 01 - Feb. 07, 2021
geometry: margin=3cm
header-includes: |
		 \usepackage{fancyhdr}
		 \pagestyle{fancy}
		 \usepackage{mathrsfs}
		 \usepackage{amssymb}
		 \usepackage{amsmath}
output: pdf_document
--- -->
<!-- &nbsp;&nbsp;  -->

<!-- <script type="text/x-mathjax-config"> MathJax.Hub.Config({tex2jax: { inlineMath:[['$','$'], ['\\(','\\)']],processEscapes: true},jax: ["input/TeX","input/MathML","input/AsciiMath","output/CommonHTML"],extensions: ["tex2jax.js","mml2jax.js","asciimath2jax.js","MathMenu.js","MathZoom.js","AssistieMML.js", "[Contrib]/a11y/accessibility-menu.js"],TeX: {extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"],equationNumbers: {autoNumber: "AMS"}}});</script> -->


## Overview
During the first week of the spring semester for NSCI607-06, I thought it would be a good idea to take inventory of everything I had from last semester and to create an outline for (1) what I should be learning, (2) what I need to program, and (3) what other things I might require for things to work. So, the next section focuses on what I currently have "done" and the next section details some logistical aspects for the completion of this project. At the end there is a section with my intentions for next week, the week of Feb. 07 to Feb. 13.

## Goals

1. Create a clean interface for ant researchers to extract ant path data from their videos of ants.
2. Use object detection to detect the paths ants take accurately.
3. Learn more about the intricacies of the $wx$ package in Python.
4. Learn more about OpenCV and object detection.
5. Gain skills in building decently complex things.

## Current State

1. A folder containing everything needed to run YOLOv3. There is a significant quantity of files that must be understood to run YOLOv3. I am a little bit rusty with the whole process but should remember quickly.
2. In order to apply the results of YOLOv3, there exists an IPython Notebook that uses the trained YOLOv3 to detect ants and works.
3. I have trained YOLOv3 on one ant video and used it to detect the ant in the same video.
4. I have just made a repository on GitHub with some of the more important files.

## TODO

1. Retrain YOLOv3 on a variety of ant videos and apply detection on a variety of ant videos.
2. Extract the path of the ants (probably via the bounding box) in each video.
3. Transition to YOLOv4 (this might not be necessary).
4. Make the detection more robust (no lapsing out of bounding box).
5. Create the GUI for the ant detection.
6. Create a tutorial on the training process works with YOLOv3.

## Current G. Colab Program for YOLOv3

```
from google.colab import drive
drive.mount('/content/drive')

!unzip "/content/drive/My Drive/darknet.zip"

%cd /content/darknet
!make
!chmod +x ./darknet

!rm /content/darknet/backup -r
!ln -s /content/drive/'My Drive'/backup /content/darknet

!sudo apt install dos2unix

!dos2unix ./data/train.txt
!dos2unix ./data/test.txt
!dos2unix ./data/obj.data
!dos2unix ./data/obj.names
!dos2unix ./cfg/yolo-obj.cfg

%cd /content/darknet
!./darknet detector train data/obj.data cfg/yolo-obj.cfg darknet53.conv.74 -dont_show
```

## Current G. Colab Program for Application
```
from google.colab import drive
drive.mount('/content/drive')

!unzip '/content/drive/My Drive/darknet.zip'

%cd /content/darknet
!cp /content/drive/'My Drive'/backup/yolo-obj_2000trial03y3.weights /content/darknet

!pip3 uninstall -y opencv_python
!pip2 uninstall -y opencv_python
!pip3 uninstall -y opencv
!pip2 uninstall -y opencv
!pip3 uninstall -y cv2
!pip2 uninstall -y cv2
!pip uninstall opencv_python -y

!pip3 install opencv_python==3.4.7.28

import numpy as np
import time
import cv2
import os
import matplotlib.pyplot as plt
%matplotlib inline

cv2.__version__

def display_img(img,cmap=None):
    fig = plt.figure(figsize = (12,12))
    plt.axis(False)
    ax = fig.add_subplot(111)
    ax.imshow(img,cmap)

labelsPath = os.path.join("/content/darknet/data/obj.names")
LABELS = open(labelsPath).read().strip().split("\n")
weightsPath = os.path.join("/content/darknet/yolo-obj_2000trial03y3.weights")
configPath = os.path.join("/content/darknet/cfg/yolo-obj.cfg")

net = cv2.dnn.readNetFromDarknet(configPath, weightsPath)

def predict(image):

    # initialize a list of colors to represent each possible class label
    np.random.seed(42)
    COLORS = np.random.randint(0, 255, size=(len(LABELS), 3), dtype="uint8")
    (H, W) = image.shape[:2]

    # determine only the "ouput" layers name which we need from YOLO
    ln = net.getLayerNames()
    ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]

    # construct a blob from the input image and then perform a forward pass of the YOLO object detector,
    # giving us our bounding boxes and associated probabilities
    blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416), swapRB=True, crop=False)
    net.setInput(blob)
    layerOutputs = net.forward(ln)

    boxes = []
    confidences = []
    classIDs = []
    threshold = 0.2

    # loop over each of the layer outputs
    for output in layerOutputs:
        # loop over each of the detections
        for detection in output:
            # extract the class ID and confidence (i.e., probability) of
            # the current object detection
            scores = detection[5:]
            classID = np.argmax(scores)
            confidence = scores[classID]

            # filter out weak predictions by ensuring the detected
            # probability is greater than the minimum probability
            # confidence type=float, default=0.5
            if confidence > threshold:
                # scale the bounding box coordinates back relative to the
                # size of the image, keeping in mind that YOLO actually
                # returns the center (x, y)-coordinates of the bounding
                # box followed by the boxes' width and height
                box = detection[0:4] * np.array([W, H, W, H])
                (centerX, centerY, width, height) = box.astype("int")

                # use the center (x, y)-coordinates to derive the top and
                # and left corner of the bounding box
                x = int(centerX - (width / 2))
                y = int(centerY - (height / 2))

                # update our list of bounding box coordinates, confidences,
                # and class IDs
                boxes.append([x, y, int(width), int(height)])
                confidences.append(float(confidence))
                classIDs.append(classID)

    # apply non-maxima suppression to suppress weak, overlapping bounding boxes
    idxs = cv2.dnn.NMSBoxes(boxes, confidences, threshold, 0.1)

    # ensure at least one detection exists
    if len(idxs) > 0:
        # loop over the indexes we are keeping
        for i in idxs.flatten():
            # extract the bounding box coordinates
            (x, y) = (boxes[i][0], boxes[i][1])
            (w, h) = (boxes[i][2], boxes[i][3])

            # draw a bounding box rectangle and label on the image
            color = (255,0,0)
            cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)
            text = "{}".format(LABELS[classIDs[i]], confidences[i])
            cv2.putText(image, text, (x +15, y - 10), cv2.FONT_HERSHEY_SIMPLEX,
                1, color, 2)
    return image

img = cv2.imread("/content/darknet/data/obj/output_1096.jpg")
img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
display_img(predict(img))
```
